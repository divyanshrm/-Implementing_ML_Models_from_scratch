{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "missing-reminder",
   "metadata": {},
   "source": [
    "## SEMINAR IMPLEMENTATION\n",
    "### Name and Roll no-Divyansh Singh,18ucs127\n",
    "### Name and Roll no-Lakshay Bhagtani,18ucs132"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-timing",
   "metadata": {},
   "source": [
    "Note- This pdf is searchable and is generated using pyppeteer. ipynb to pdf converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ancient-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "conservative-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE WE ARE LOADING THE DATASET FROM RAW TEXT FILES USING NUMPY\n",
    "x_train=np.loadtxt('p1_a_X.dat')\n",
    "y_train=np.loadtxt('p1_a_y.dat')\n",
    "x_test=np.loadtxt('p1_b_X.dat')\n",
    "y_test=np.loadtxt('p1_b_y.dat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "british-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the minimum and maximum elements for each feature in the dataset.\n",
    "max_0=max(np.hsplit(x_train,2)[0])\n",
    "max_1=max(np.hsplit(x_train,2)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "resistant-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_1=min(np.hsplit(x_train,2)[1])\n",
    "min_0=min(np.hsplit(x_train,2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "small-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This normalizer performs MIN-MAX scaling on the dataset.\n",
    "\n",
    "def normalizer(data,min_x,min_y,range_x,range_y):\n",
    "    x=np.hsplit(data,2)[0]\n",
    "    y=np.hsplit(data,2)[1]\n",
    "    x=(x-min_x)/range_x\n",
    "    y=(y-min_y)/range_y\n",
    "    return np.hstack((x,y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "collectible-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing the dataset\n",
    "x_train=normalizer(x_train,min_0,min_1,max_0-min_0,max_1-min_1)\n",
    "x_test=normalizer(x_test,min_0,min_1,max_0-min_0,max_1-min_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hairy-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS FUNCTION CONVERTS THE CLASSES INTO 0 and 1,\n",
    "def label_binarizer(data):\n",
    "    for x in range(len(data)):\n",
    "        if data[x]==1:\n",
    "            data[x]=1\n",
    "        if data[x]==-1:\n",
    "            data[x]=0\n",
    "    return data\n",
    "#we are now binarizing our training and testing labels\n",
    "y_train=label_binarizer(y_train)\n",
    "y_test=label_binarizer(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-violin",
   "metadata": {},
   "source": [
    "### This function calculates the performance metrics and returns Accuracy, Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "worldwide-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This funciton returns the Accuracy, Precision and recall from the predicted and true labels.\n",
    "def metrics(y_true,predicted):\n",
    "    true_positives=0\n",
    "    correct=0\n",
    "    false_positives=0\n",
    "    false_negatives=0\n",
    "    for x in range(len(y_true)):\n",
    "        if y_true[x]==predicted[x]:\n",
    "            correct+=1.0\n",
    "        if y_true[x]==1 and predicted[x]==1:\n",
    "            true_positives+=1.0\n",
    "        if y_true[x]==0 and predicted[x]==1:\n",
    "            false_positives+=1.0\n",
    "        if y_true[x]==1 and predicted[x]==0:\n",
    "            false_negatives+=1.0\n",
    "    accuracy=(correct*100)/len(y_true)\n",
    "    preci=(true_positives)/(true_positives+false_positives)\n",
    "    recall=(true_positives)/(true_positives+false_negatives)\n",
    "    print('Accuracy is '+str(accuracy)+' ,Precision is '+str(preci)+' ,Recall is '+str(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-cathedral",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-combining",
   "metadata": {},
   "source": [
    "We will Implement Logistic Regression below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-marshall",
   "metadata": {},
   "source": [
    "This function performs the sigmoid activation on an array/scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sustainable-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    s = 1.0/(1.0 + np.exp(-1.0 * x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-personality",
   "metadata": {},
   "source": [
    "We will initialize the weights and then use gradient descent to minimize the logistic loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "interstate-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_regression_train(X, y):\n",
    "#Here we initialize the learning rate as 0.1 and epochs to 10000\n",
    "    learning_rate=0.1\n",
    "    epochs=10000\n",
    "#we are initializing the theta and bias, the number of theta would be 2 as we have 2 dimensional input and 1 bias weight\n",
    "    theta = np.zeros(2)\n",
    "    b=0\n",
    "    #total number of examples in triaining\n",
    "    m=X.shape[0]\n",
    "   \n",
    "    #This function returns the activation given the inputs\n",
    "    def forward(theta,b, inputs):\n",
    "        #This code applies matrix dot product to theta and input and adds the bias term\n",
    "        Z= np.dot(inputs,theta)+b\n",
    "        A = sigmoid(Z)\n",
    "        return A\n",
    "       \n",
    "#Here we are training the model and initializeing the for loop to run for the number of epochs and updates as 0\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        #First we compute the predicted output from the input \n",
    "        A = forward(theta,b,X)\n",
    "#we calculate the cost\n",
    "        cost =(-1.0)*np.mean(np.multiply(y, np.log(A))+np.multiply(1-y, np.log(1- A)))\n",
    "#we calculate the gradient for each data point\n",
    "        dw = np.dot(X.T, (A - y)) * (1.0/m)\n",
    "        db = np.mean((A - y))\n",
    "#Then we update the theta with the gradient\n",
    "        theta = theta-learning_rate * dw\n",
    "        b = b-learning_rate * db\n",
    "#printing loss every 100 epochs\n",
    "        if (i)%1000==0:\n",
    "            print('The Loss at epoch '+str(i)+' is '+str(cost))\n",
    "    print('Final Loss is '+ str(cost))\n",
    "    return theta,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-bulletin",
   "metadata": {},
   "source": [
    "This function will perform prediction on the test data by using the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "standard-brave",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_test(theta,b,x_test):\n",
    "#This function predicts the labels using the weights\n",
    "    prediction=[]\n",
    "    Z= np.matmul(x_test, theta) + b\n",
    "    A = sigmoid(Z)\n",
    "    y_predicted=np.asarray(A)\n",
    "    for x in A:\n",
    "        if x>=0.5:\n",
    "            prediction.append(1.0)\n",
    "        else:\n",
    "            prediction.append(0.)\n",
    "    return np.asarray(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-aaron",
   "metadata": {},
   "source": [
    "We will now use the above functions to train our model on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "boxed-greek",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Loss at epoch 0 is 0.6931471805599454\n",
      "The Loss at epoch 1000 is 0.29327497490915283\n",
      "The Loss at epoch 2000 is 0.19430155328815277\n",
      "The Loss at epoch 3000 is 0.14931900508675414\n",
      "The Loss at epoch 4000 is 0.12312698381250668\n",
      "The Loss at epoch 5000 is 0.10576143222930168\n",
      "The Loss at epoch 6000 is 0.09329226741338258\n",
      "The Loss at epoch 7000 is 0.08384310366844605\n",
      "The Loss at epoch 8000 is 0.07639885604370428\n",
      "The Loss at epoch 9000 is 0.070359624679729\n",
      "Final Loss is 0.06535128683517027\n",
      "The theta are [-13.11048198  13.35124297] and bias is 0.09085872891935888\n"
     ]
    }
   ],
   "source": [
    "#We train our perceptron and then then initialise the theta for testing the model.\n",
    "theta,b=Logistic_regression_train(x_train,y_train)\n",
    "print('The theta are '+ str(theta)+' and bias is '+str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-fever",
   "metadata": {},
   "source": [
    "We will now evaluate the performance of the model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "structured-copper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Logistic Regression Model on the Test Data: \n",
      "Accuracy is 100.0 ,Precision is 1.0 ,Recall is 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Performance of Logistic Regression Model on the Test Data: ')\n",
    "metrics(y_train,logistic_test(theta,b,x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-andrew",
   "metadata": {},
   "source": [
    "# KNN MODEL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-router",
   "metadata": {},
   "source": [
    "We will now implement KNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "absolute-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_implementation(x_train,labels,test_data,k):    \n",
    "#function to calculate the distance between two points\n",
    "    def calculate_distance(point1,point2):\n",
    "        y2=point2[1]\n",
    "        y1=point1[1]\n",
    "        x2=point2[0]\n",
    "        x1=point1[0]\n",
    "        return pow(pow(y2-y1,2)+pow(x2-x1,2),1/2)\n",
    "#this function returns the distances of all the elements in a vector from  a given point\n",
    "    def get_distances(point,data):\n",
    "        distance=[]\n",
    "        for x in data:\n",
    "            s=calculate_distance(point,x)\n",
    "            distance.append(s)\n",
    "        return np.asarray(distance)\n",
    "#this function sorts the distances and returns the labels of nearest data points\n",
    "    def get_nearest_points(point,data,labels,nearest_points=1):\n",
    "        distances=get_distances(point,data)\n",
    "        sorted_index=np.argsort(distances) \n",
    "        return labels[sorted_index[:nearest_points]]\n",
    "#returns the frequency distribution of 0s and 1s in the array\n",
    "    def counter(data):\n",
    "        counter=[0,0]\n",
    "        for x in data:\n",
    "            if x==1:\n",
    "                counter[1] +=1\n",
    "            if x==0:\n",
    "                counter[0] +=1 \n",
    "        return counter\n",
    "#this function predicts the values based on counter and breaks the ties if there are any.\n",
    "    def KNN(test_data,training_data,labels,k):\n",
    "        predictions=[]\n",
    "        for x in test_data:\n",
    "            nearest_point_labels=get_nearest_points(x,training_data,labels,k)\n",
    "            counter_prediction=counter(nearest_point_labels)\n",
    "            #In case of a tie we break the tie by searching for a lower k value\n",
    "            while(counter_prediction[0]==counter_prediction[1]):\n",
    "                nearest_point_labels=get_nearest_points(x,training_data,labels,k-1)\n",
    "                counter_prediction=counter(nearest_point_labels)\n",
    "            if counter_prediction[0]>counter_prediction[1]:\n",
    "                predictions.append(0)\n",
    "            else:\n",
    "                predictions.append(1)\n",
    "        return np.asarray(predictions)\n",
    "            \n",
    "    return(KNN(x_test,x_train,y_train,k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-clarity",
   "metadata": {},
   "source": [
    "We will now evaluate the performance of the model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "directed-reverse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of KNN model on the Test Data: \n",
      "The Metrics of performance on the test set for K value:1 are: \n",
      "Accuracy is 99.95 ,Precision is 1.0 ,Recall is 0.999\n",
      "The Metrics of performance on the test set for K value:2 are: \n",
      "Accuracy is 99.95 ,Precision is 1.0 ,Recall is 0.999\n",
      "The Metrics of performance on the test set for K value:3 are: \n",
      "Accuracy is 99.95 ,Precision is 1.0 ,Recall is 0.999\n",
      "The Metrics of performance on the test set for K value:4 are: \n",
      "Accuracy is 99.95 ,Precision is 1.0 ,Recall is 0.999\n"
     ]
    }
   ],
   "source": [
    "print('Performance of KNN model on the Test Data: ')\n",
    "for x in range(1,5):\n",
    "    print('The Metrics of performance on the test set for K value:'+str(x)+ ' are: ')\n",
    "    metrics(y_test,KNN_implementation(x_train,y_train,x_test,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-serve",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-print",
   "metadata": {},
   "source": [
    "We will now implement an Artificial Neural Network\n",
    "This model will have only 1 hidden layer to maintain simplicity.\n",
    "The size of this layer is variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "designing-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will initialize the weights as random numbers to avoid explosion of weights.\n",
    "def parameters_initialize(hidden_layer):\n",
    "    w1= np.random.randn(hidden_layer,2)*0.01\n",
    "    b1 = np.zeros((hidden_layer,1))\n",
    "    w2 = np.random.randn(1,hidden_layer)*0.01\n",
    "    b2 = np.zeros((1,1))\n",
    "    return w1,b1,w2,b2\n",
    "#This function returns the activations by forward propagation in the network\n",
    "def forward(X,w1,b1,w2,b2):\n",
    "    z1=np.dot(w1,X.T)\n",
    "    z1=z1+b1\n",
    "    a1=np.tanh(z1)\n",
    "    z2=np.dot(w2,a1)\n",
    "    z2=z2+b2\n",
    "    a2=sigmoid(z2)\n",
    "    return z1,a1,z2,a2\n",
    "\n",
    "\n",
    "#this function calculates the cost.\n",
    "def compute_cost(a2,y):\n",
    "    cost = (-1.0) * np.mean(np.multiply(y.T, np.log(a2)) + np.multiply(1-y.T, np.log(1- a2)))\n",
    "    return cost\n",
    "#this function calculates the gradients for each weight and then updates them given the learning rate\n",
    "def back_prop_and_update(w1,b1,w2,b2,X,y,learning_rate):\n",
    "    z1,a1,z2,a2=forward(X,w1,b1,w2,b2)\n",
    "    m=X.shape[0]\n",
    "    \n",
    "    dz2 = a2-y.T\n",
    "    dw2 = 1/m*np.dot(dz2, a1.T)\n",
    "    db2 = 1/m*np.sum(dz2, axis=1, keepdims=True)\n",
    "    dz1 = np.dot(w2.T, dz2)*(1 - np.power(a1, 2))\n",
    "    dw1 = 1/m*np.dot(dz1, X)\n",
    "    db1 = 1/m*np.sum(dz1, axis=1, keepdims=True)\n",
    "#updating the parameters\n",
    "    w1=w1-learning_rate*dw1\n",
    "    w2=w2-learning_rate*dw2\n",
    "    b1=b1-learning_rate*db1\n",
    "    b2=b2-learning_rate*db2\n",
    "    return w1,w2,b1,b2\n",
    "    \n",
    "#This function performs forward and backprop till the number of epochs        \n",
    "\n",
    "def nn_model(X,y,hidden_layer,learning_rate=0.1,epochs=1000):\n",
    "    w1,b1,w2,b2=parameters_initialize(hidden_layer)\n",
    "    for x in range(epochs):\n",
    "        w1,w2,b1,b2=back_prop_and_update(w1,b1,w2,b2,X,y,learning_rate)\n",
    "#printing loss every 100 epochs\n",
    "        if(x%100==0):\n",
    "            _,_,_,a2=forward(X,w1,b1,w2,b2)\n",
    "            print('The Loss in Epoch- '+str(x)+ ' is '+str(compute_cost(a2,y)))\n",
    "    _,_,_,a2=forward(X,w1,b1,w2,b2)\n",
    "    print('Final loss is '+ str(compute_cost(a2,y)))\n",
    "    return w1,b1,w2,b2\n",
    "\n",
    "#this function performs predictions given the weights and test data\n",
    "\n",
    "def nn_predict(x_test,w1,w2,b1,b2):\n",
    "    predictions=[]\n",
    "    _,_,_,a2=forward(x_test,w1,b1,w2,b2)\n",
    "    a2=a2.T\n",
    "    for x in a2:\n",
    "        if(x>=0.5):\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return np.asarray(predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "later-camcorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Loss in Epoch- 0 is 0.6931045454679674\n",
      "The Loss in Epoch- 100 is 0.6927519023914354\n",
      "The Loss in Epoch- 200 is 0.6904529959205145\n",
      "The Loss in Epoch- 300 is 0.6753610332204724\n",
      "The Loss in Epoch- 400 is 0.5929800660987165\n",
      "The Loss in Epoch- 500 is 0.3780435197291027\n",
      "The Loss in Epoch- 600 is 0.20436249730573244\n",
      "The Loss in Epoch- 700 is 0.12480760083972182\n",
      "The Loss in Epoch- 800 is 0.08646645426749958\n",
      "The Loss in Epoch- 900 is 0.06512687425120008\n",
      "Final loss is 0.051943915222767444\n"
     ]
    }
   ],
   "source": [
    "w1,b1,w2,b2=nn_model(x_train,y_train,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "according-hotel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Neural Network Model on the Test Data: \n",
      "Accuracy is 99.9 ,Precision is 0.999 ,Recall is 0.999\n"
     ]
    }
   ],
   "source": [
    "print('Performance of Neural Network Model on the Test Data: ')\n",
    "metrics(y_test,nn_predict(x_test,w1,w2,b1,b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-bobby",
   "metadata": {},
   "source": [
    "# PERFORMANCE COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "czech-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "amateur-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('per.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "overhead-waterproof",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unnamed: 0 Logistic Regression     KNN    ANN\n",
      "0   Accuracy                100%  99.95%  99.9%\n",
      "1  Precision                 1.0     1.0  0.999\n",
      "2     Recall                 1.0   0.999  0.999\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-alarm",
   "metadata": {},
   "source": [
    "The Reason behind the lower accuracy of ANN is likely because of overfitting and lack of regularization.\n",
    "As logistic regression is simply a single node neural network.\n",
    "Generally ANN are able to generalize more complex decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-change",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
